{
	"corpora": {
		"text8": {
			"description": "Cleaned small sample from wikipedia",
			"checksum": "f407f5aed497fc3b0fb33b98c4f9d855",
			"file_name": "text8",
			"source": "http://mattmahoney.net/dc/text8.zip",
			"parts": 1
		},
		"fake-news": {
			"description": "It contains text and metadata scraped from 244 websites tagged as 'bullshit' here by the BS Detector Chrome Extension by Daniel Sieradski.",
			"checksum": "a61f985190ba361defdfc3fef616b9cd",
			"file_name": "fake.csv",
			"source": "Kaggle",
			"parts": 1
		},
		"20-newsgroups": {
			"description": "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups",
			"checksum": "d6e9e45cb8cb77ec5276dfa6dfc14318",
			"file_name": "20-newsgroups",
			"source": "http://qwone.com/~jason/20Newsgroups/",
			"parts": 1
		},
		"__testing_matrix-synopsis": {
			"description": "Synopsis of the movie matrix",
			"checksum": "25a12bdb4d037c560ec69602becaa413",
			"file_name": "matrix-synopsis.txt",
			"source": "http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis",
			"parts": 1
		},
		"__testing_multipart-matrix-synopsis": {
			"description": "Synopsis of the movie matrix",
			"checksum-0": "1b907374c90aae19bbf6bc7676b43dbb",
			"checksum-1": "4621c2a340b11a74785e5c91e6e4484a",
			"checksum-2": "31d8484043fd092875820076c2d97b1d",
			"file_name": "matrix-synopsis.txt",
			"source": "http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis",
			"parts": 3
		}
	},
	"models": {
		"word2vec-google-news-300": {
			"description": "Pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality', https://code.google.com/archive/p/word2vec/",
			"parameters": "dimension = 300",
			"papers": "https://arxiv.org/abs/1301.3781, https://arxiv.org/abs/1310.4546, https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf",
			"checksum": "b269a9c8b16c3178d3d0651fd68c3fe9",
			"file_name": "GoogleNews-vectors-negative300.bin",
			"parts": 1
		},
		"glove-wiki-gigaword-50": {
			"description": "Pre-trained vectors ,Wikipedia 2014 + Gigaword 5,6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/",
			"parameters": "dimension = 50",
			"preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`",
			"papers": "https://nlp.stanford.edu/pubs/glove.pdf",
			"checksum": "b269a9c8b16c3178d3d0651fd68c3fe9",
			"file_name": "glove-wiki-gigaword-50.txt",
			"parts": 1
		},
		"glove-wiki-gigaword-100":{
			"description": "Pre-trained vectors ,Wikipedia 2014 + Gigaword 5,6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/",
			"parameters": "dimensions = 100",
			"preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`",
			"papers": "https://nlp.stanford.edu/pubs/glove.pdf",
			"checksum": "980b245345a7ae79e74fe1a3fb995278",
			"file_name": "glove-wiki-gigaword-100.txt",
			"parts": 1
		},
		"glove-wiki-gigaword-200":{
			"description": "Pre-trained vectors ,Wikipedia 2014 + Gigaword 5,6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/",
			"parameters": "dimentions = 200",
			"preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`",
			"papers": "https://nlp.stanford.edu/pubs/glove.pdf",
			"checksum": "dd3a148274c20935ebd541f4381e6043",
			"file_name": "glove-wiki-gigaword-200.txt",
			"parts": 1
		},
		"glove-wiki-gigaword-300":{
			"description": "Pre-trained vectors, Wikipedia 2014 + Gigaword 5, 6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/",
			"parameters": "dimensions = 300",
			"preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`",
			"papers": "https://nlp.stanford.edu/pubs/glove.pdf",
			"checksum": "c6e713332ed2fd42b109f7f9fd42a3e6",
			"file_name": "glove-wiki-gigaword-300.txt",
			"parts": 1
		},
		"glove-twitter-25":{
			"description": "Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/",
			"parameters": "dimensions = 25",
			"preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`",
			"papers": "https://nlp.stanford.edu/pubs/glove.pdf",
			"checksum": "5c3784f59a8a7761059342dc32f8f9e6",
			"file_name": "glove-twitter-25.txt",
			"parts": 1
		},
		"glove-twitter-50":{
			"description": "Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/",
			"parameters": "dimensions = 50",
			"preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`",
			"papers": "https://nlp.stanford.edu/pubs/glove.pdf",
			"checksum": "42b16fc98c60059ae309501b8e0cb668",
			"file_name": "glove-twitter-50.txt",
			"parts": 1
		},
		"glove-twitter-100":{
			"description": "Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/",
			"parameters": "dimensions = 100",
			"preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`",
			"papers": "https://nlp.stanford.edu/pubs/glove.pdf",
			"checksum": "31b964e13ca7f8769b648b506b119c76",
			"file_name": "glove-twitter-100.txt",
			"parts": 1
		},
		"glove-twitter-200":{
			"description": "Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/",
			"parameters": "dimensions = 200",
			"preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`",
			"papers": "https://nlp.stanford.edu/pubs/glove.pdf",
			"checksum": "7f9cf73835ec82acdbb87e5792f9365e",
			"file_name": "glove-twitter-200.txt",
			"parts": 1
		},
		"__testing_word2vec-matrix-synopsis":{
			"description": "Word vecrors of the movie matrix",
			"parameters": "dimentions = 50",
			"preprocessing": "Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`",
			"papers": "",
			"checksum": "704ec028f0c90216582ea874b6a6ed99",
			"file_name": "word2vec-matrix-synopsis.txt",
			"parts": 1
		}
	}
}
